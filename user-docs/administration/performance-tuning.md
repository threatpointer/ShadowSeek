# Performance Tuning

## âš¡ ShadowSeek Performance Optimization

This guide covers performance optimization strategies for all components of the ShadowSeek platform.

---

## ðŸŽ¯ **Performance Overview**

### **System Performance Metrics**
```mermaid
graph TB
    subgraph "Frontend Performance"
        FP1[Bundle Size < 2MB]
        FP2[Initial Load < 3s]
        FP3[Time to Interactive < 5s]
        FP4[Core Web Vitals]
    end
    
    subgraph "Backend Performance"
        BP1[API Response < 200ms]
        BP2[Database Query < 100ms]
        BP3[Memory Usage < 4GB]
        BP4[CPU Usage < 80%]
    end
    
    subgraph "Analysis Performance"
        AP1[Binary Analysis < 30s]
        AP2[Function Analysis < 5s]
        AP3[AI Analysis < 10s]
        AP4[Pattern Matching < 1s]
    end
    
    subgraph "Fuzzing Performance"
        FZP1[Harness Generation < 60s]
        FZP2[Fuzzing Speed > 100 exec/s]
        FZP3[Crash Processing < 5s]
        FZP4[Coverage Analysis < 10s]
    end
```

### **Performance Targets**
| Component | Metric | Target | Acceptable |
|-----------|--------|--------|------------|
| Frontend | Initial Load | < 2s | < 3s |
| API | Response Time | < 100ms | < 200ms |
| Database | Query Time | < 50ms | < 100ms |
| Binary Analysis | Processing Time | < 15s | < 30s |
| AI Analysis | Response Time | < 5s | < 10s |
| Fuzzing | Execution Speed | > 500 exec/s | > 100 exec/s |

---

## ðŸ–¥ï¸ **Frontend Performance Optimization**

### **Bundle Optimization**
```javascript
// webpack.config.js optimization
module.exports = {
  optimization: {
    splitChunks: {
      chunks: 'all',
      cacheGroups: {
        vendor: {
          test: /[\\/]node_modules[\\/]/,
          name: 'vendors',
          chunks: 'all',
        },
        common: {
          name: 'common',
          minChunks: 2,
          chunks: 'all',
          enforce: true,
        },
      },
    },
    usedExports: true,
    sideEffects: false,
  },
  resolve: {
    alias: {
      '@': path.resolve(__dirname, 'src'),
    },
  },
};
```

### **Code Splitting and Lazy Loading**
```javascript
// Lazy load components
const BinaryDetails = lazy(() => import('./BinaryDetails'));
const FunctionAnalysis = lazy(() => import('./FunctionAnalysis'));
const SecurityDashboard = lazy(() => import('./SecurityDashboard'));
const FuzzingDashboard = lazy(() => import('./FuzzingDashboard'));

// Route-based code splitting
const Router = () => (
  <Suspense fallback={<Loading />}>
    <Routes>
      <Route path="/binary/:id" element={<BinaryDetails />} />
      <Route path="/function/:id" element={<FunctionAnalysis />} />
      <Route path="/security" element={<SecurityDashboard />} />
      <Route path="/fuzzing" element={<FuzzingDashboard />} />
    </Routes>
  </Suspense>
);
```

### **React Performance Optimization**
```javascript
// Memoization for expensive components
const ExpensiveComponent = memo(({ data, filters }) => {
  const filteredData = useMemo(() => {
    return data.filter(item => 
      filters.every(filter => filter.test(item))
    );
  }, [data, filters]);
  
  return <DataTable data={filteredData} />;
});

// Virtualization for large lists
import { FixedSizeList } from 'react-window';

const VirtualizedList = ({ items }) => (
  <FixedSizeList
    height={600}
    itemCount={items.length}
    itemSize={50}
    width="100%"
  >
    {({ index, style }) => (
      <div style={style}>
        {items[index]}
      </div>
    )}
  </FixedSizeList>
);
```

### **Asset Optimization**
```bash
# Image optimization
npm install --save-dev imagemin imagemin-webp imagemin-svgo

# Configure in build process
const imagemin = require('imagemin');
const imageminWebp = require('imagemin-webp');

(async () => {
  await imagemin(['src/assets/images/*.{jpg,png}'], {
    destination: 'public/images',
    plugins: [
      imageminWebp({quality: 80})
    ]
  });
})();
```

---

## ðŸ”§ **Backend Performance Optimization**

### **Flask Application Optimization**
```python
# app.py - Production configuration
from flask import Flask
from flask_caching import Cache
from flask_compress import Compress

app = Flask(__name__)

# Enable compression
compress = Compress(app)

# Configure caching
cache = Cache(app, config={
    'CACHE_TYPE': 'redis',
    'CACHE_REDIS_URL': 'redis://localhost:6379/0',
    'CACHE_DEFAULT_TIMEOUT': 300
})

# Optimize JSON responses
app.config['JSON_SORT_KEYS'] = False
app.config['JSONIFY_PRETTYPRINT_REGULAR'] = False

# Configure request handling
@app.before_request
def before_request():
    g.start_time = time.time()

@app.after_request
def after_request(response):
    # Add performance headers
    response.headers['X-Response-Time'] = f"{time.time() - g.start_time:.3f}s"
    return response
```

### **Database Query Optimization**
```python
# Optimize database queries
from sqlalchemy import text
from sqlalchemy.orm import selectinload, joinedload

# Use eager loading to prevent N+1 queries
def get_binary_with_functions(binary_id):
    return Binary.query.options(
        selectinload(Binary.functions),
        selectinload(Binary.security_findings)
    ).get(binary_id)

# Use raw SQL for complex queries
def get_analysis_statistics():
    query = text("""
        SELECT 
            file_type,
            COUNT(*) as total_binaries,
            AVG(
                CASE WHEN completed_at IS NOT NULL AND started_at IS NOT NULL
                THEN EXTRACT(EPOCH FROM (completed_at - started_at))
                ELSE NULL END
            ) as avg_analysis_time
        FROM binary b
        JOIN task t ON b.id = t.binary_id
        WHERE t.task_type = 'binary_analysis'
        AND t.status = 'completed'
        GROUP BY file_type
    """)
    
    return db.session.execute(query).fetchall()

# Implement pagination for large datasets
def get_paginated_binaries(page=1, per_page=50):
    return Binary.query.paginate(
        page=page,
        per_page=per_page,
        error_out=False
    )
```

### **Caching Strategy**
```python
# Redis caching implementation
import redis
import json
from functools import wraps

redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)

def cache_result(timeout=300):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Create cache key
            cache_key = f"{func.__name__}:{hash(str(args) + str(kwargs))}"
            
            # Try to get from cache
            cached_result = redis_client.get(cache_key)
            if cached_result:
                return json.loads(cached_result)
            
            # Execute function
            result = func(*args, **kwargs)
            
            # Cache result
            redis_client.setex(
                cache_key,
                timeout,
                json.dumps(result, default=str)
            )
            
            return result
        return wrapper
    return decorator

# Apply caching to expensive operations
@cache_result(timeout=600)
def get_security_analysis_summary(binary_id):
    return SecurityFinding.query.filter_by(binary_id=binary_id).all()
```

### **API Response Optimization**
```python
# Optimize API responses
from flask import jsonify, request
from marshmallow import Schema, fields

class BinarySchema(Schema):
    id = fields.Int()
    name = fields.Str()
    size = fields.Int()
    md5_hash = fields.Str()
    file_type = fields.Str()
    analysis_status = fields.Str()
    created_at = fields.DateTime()
    
    # Conditionally include expensive fields
    functions = fields.Nested('FunctionSchema', many=True, exclude=['decompiled_code'])
    security_findings = fields.Nested('SecurityFindingSchema', many=True)

# Implement field selection
@app.route('/api/binary/<int:binary_id>')
def get_binary(binary_id):
    binary = Binary.query.get_or_404(binary_id)
    
    # Parse fields parameter
    fields = request.args.get('fields', '').split(',')
    
    # Serialize with field selection
    schema = BinarySchema()
    if fields and fields[0]:
        schema = BinarySchema(only=fields)
    
    return jsonify(schema.dump(binary))
```

---

## ðŸ—„ï¸ **Database Performance Optimization**

### **Query Optimization**
```sql
-- Optimize common queries with proper indexing
-- Index for binary lookups
CREATE INDEX CONCURRENTLY idx_binary_lookup ON binary(md5_hash, sha256_hash);

-- Index for function analysis
CREATE INDEX CONCURRENTLY idx_function_analysis ON function(binary_id, function_type);

-- Index for security findings
CREATE INDEX CONCURRENTLY idx_security_findings_lookup ON security_finding(binary_id, severity, confidence);

-- Composite index for common filters
CREATE INDEX CONCURRENTLY idx_binary_filtered ON binary(file_type, analysis_status, created_at);

-- Partial index for active tasks
CREATE INDEX CONCURRENTLY idx_active_tasks ON task(status, task_type) WHERE status IN ('pending', 'running');
```

### **Connection Pool Optimization**
```python
# Database connection pool configuration
DATABASE_CONFIG = {
    'pool_size': 20,              # Base pool size
    'max_overflow': 30,           # Additional connections
    'pool_timeout': 30,           # Connection timeout
    'pool_recycle': 3600,         # Recycle connections every hour
    'pool_pre_ping': True,        # Validate connections
    'echo': False,                # Disable query logging in production
    'connect_args': {
        'connect_timeout': 10,
        'command_timeout': 60,
        'server_settings': {
            'jit': 'off',          # Disable JIT for simple queries
            'application_name': 'shadowseek'
        }
    }
}
```

### **Query Performance Monitoring**
```python
# Query performance monitoring
import time
from sqlalchemy import event
from sqlalchemy.engine import Engine

@event.listens_for(Engine, "before_cursor_execute")
def before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    conn.info.setdefault('query_start_time', []).append(time.time())

@event.listens_for(Engine, "after_cursor_execute")
def after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    total = time.time() - conn.info['query_start_time'].pop(-1)
    
    # Log slow queries
    if total > 0.1:  # 100ms threshold
        app.logger.warning(f"Slow query: {total:.3f}s - {statement[:100]}...")
```

---

## ðŸ¤– **AI Service Optimization**

### **Request Batching**
```python
# Batch AI requests for efficiency
class AIAnalysisBatcher:
    def __init__(self, batch_size=10, max_wait=5.0):
        self.batch_size = batch_size
        self.max_wait = max_wait
        self.queue = []
        self.futures = []
    
    async def add_request(self, request):
        future = asyncio.Future()
        self.queue.append((request, future))
        self.futures.append(future)
        
        # Process if batch is full
        if len(self.queue) >= self.batch_size:
            await self._process_batch()
        
        return await future
    
    async def _process_batch(self):
        if not self.queue:
            return
        
        batch = self.queue[:self.batch_size]
        self.queue = self.queue[self.batch_size:]
        
        # Process batch
        requests = [req for req, _ in batch]
        futures = [fut for _, fut in batch]
        
        try:
            results = await self._process_ai_batch(requests)
            for future, result in zip(futures, results):
                future.set_result(result)
        except Exception as e:
            for future in futures:
                future.set_exception(e)
    
    async def _process_ai_batch(self, requests):
        # Implementation for batch processing
        pass
```

### **Response Caching**
```python
# Cache AI responses
import hashlib
from functools import wraps

def cache_ai_response(timeout=3600):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Create cache key from function input
            input_str = str(args) + str(kwargs)
            cache_key = f"ai_response:{hashlib.md5(input_str.encode()).hexdigest()}"
            
            # Check cache
            cached = redis_client.get(cache_key)
            if cached:
                return json.loads(cached)
            
            # Execute and cache
            result = func(*args, **kwargs)
            redis_client.setex(cache_key, timeout, json.dumps(result))
            
            return result
        return wrapper
    return decorator

@cache_ai_response(timeout=7200)  # 2 hours
def analyze_function_with_ai(function_code, provider='openai'):
    # AI analysis implementation
    pass
```

### **Load Balancing AI Providers**
```python
# Distribute load across AI providers
class AILoadBalancer:
    def __init__(self):
        self.providers = {
            'openai': {'weight': 3, 'current_load': 0},
            'anthropic': {'weight': 2, 'current_load': 0},
            'ollama': {'weight': 1, 'current_load': 0}
        }
    
    def select_provider(self):
        # Weighted random selection based on current load
        total_weight = sum(
            max(1, p['weight'] - p['current_load']) 
            for p in self.providers.values()
        )
        
        import random
        rand_weight = random.uniform(0, total_weight)
        
        current_weight = 0
        for provider, config in self.providers.items():
            current_weight += max(1, config['weight'] - config['current_load'])
            if rand_weight <= current_weight:
                config['current_load'] += 1
                return provider
        
        return 'openai'  # Fallback
    
    def release_provider(self, provider):
        if provider in self.providers:
            self.providers[provider]['current_load'] = max(
                0, self.providers[provider]['current_load'] - 1
            )
```

---

## ðŸ” **Analysis Performance Optimization**

### **Ghidra Bridge Optimization**
```python
# Optimize Ghidra Bridge performance
class OptimizedGhidraBridge:
    def __init__(self):
        self.connection_pool = []
        self.max_connections = 5
        self.timeout = 300
    
    def get_connection(self):
        if self.connection_pool:
            return self.connection_pool.pop()
        
        if len(self.active_connections) < self.max_connections:
            return self._create_connection()
        
        # Wait for available connection
        return self._wait_for_connection()
    
    def analyze_binary_optimized(self, binary_path):
        conn = self.get_connection()
        try:
            # Optimize analysis settings
            conn.set_analysis_options({
                'analyzeEmbeddedMedia': False,
                'createAddressTable': False,
                'asciiStrings': True,
                'applyDataArchiveTemplate': False
            })
            
            # Perform analysis
            result = conn.analyze_binary(binary_path)
            
            # Extract only necessary information
            optimized_result = {
                'functions': [
                    {
                        'name': f.name,
                        'address': f.address,
                        'size': f.size,
                        'decompiled': f.decompiled_code if f.size < 10000 else None
                    }
                    for f in result.functions
                ],
                'strings': result.strings[:1000],  # Limit strings
                'imports': result.imports,
                'exports': result.exports
            }
            
            return optimized_result
        finally:
            self.return_connection(conn)
```

### **Parallel Analysis Processing**
```python
# Parallel processing for analysis tasks
import asyncio
from concurrent.futures import ThreadPoolExecutor

class ParallelAnalysisProcessor:
    def __init__(self, max_workers=4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.semaphore = asyncio.Semaphore(max_workers)
    
    async def process_binary(self, binary_path):
        async with self.semaphore:
            # Run CPU-intensive analysis in thread pool
            loop = asyncio.get_event_loop()
            
            # Parallel task execution
            tasks = [
                loop.run_in_executor(self.executor, self._analyze_functions, binary_path),
                loop.run_in_executor(self.executor, self._extract_strings, binary_path),
                loop.run_in_executor(self.executor, self._analyze_imports, binary_path),
                loop.run_in_executor(self.executor, self._detect_patterns, binary_path)
            ]
            
            results = await asyncio.gather(*tasks)
            
            return {
                'functions': results[0],
                'strings': results[1],
                'imports': results[2],
                'security_findings': results[3]
            }
```

---

## ðŸŽ¯ **Fuzzing Performance Optimization**

### **Fuzzing Engine Optimization**
```python
# Optimize fuzzing performance
class OptimizedFuzzingEngine:
    def __init__(self):
        self.config = {
            'afl_timeout': 5000,      # 5 seconds
            'memory_limit': 1024,     # 1GB
            'cpu_cores': 4,           # Use 4 cores
            'dictionary_size': 10000, # Limit dictionary
            'max_crashes': 100        # Stop after 100 crashes
        }
    
    def optimize_harness(self, target_function):
        # Generate optimized harness
        optimized_harness = self._generate_base_harness(target_function)
        
        # Add performance optimizations
        optimized_harness += '''
        // Performance optimizations
        #define FUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION
        #pragma GCC optimize("O3")
        #pragma GCC target("native")
        
        // Reduce system calls
        setvbuf(stdout, NULL, _IONBF, 0);
        setvbuf(stderr, NULL, _IONBF, 0);
        '''
        
        return optimized_harness
    
    def configure_fuzzer(self, fuzzer_type):
        configs = {
            'afl++': {
                'AFL_FAST_CAL': '1',
                'AFL_SKIP_CPUFREQ': '1',
                'AFL_I_DONT_CARE_ABOUT_MISSING_CRASHES': '1',
                'AFL_TMPDIR': '/tmp/afl_tmp'
            },
            'libfuzzer': {
                'max_len': '65536',
                'timeout': '5',
                'rss_limit_mb': '1024',
                'malloc_limit_mb': '1024'
            }
        }
        
        return configs.get(fuzzer_type, {})
```

### **Crash Analysis Optimization**
```python
# Optimize crash analysis
class OptimizedCrashAnalyzer:
    def __init__(self):
        self.crash_cache = {}
        self.similarity_threshold = 0.8
    
    def analyze_crash(self, crash_data):
        # Quick similarity check
        crash_hash = self._hash_crash(crash_data)
        
        if crash_hash in self.crash_cache:
            similar_crash = self.crash_cache[crash_hash]
            if self._is_similar(crash_data, similar_crash):
                return similar_crash['analysis']
        
        # Perform analysis
        analysis = self._detailed_crash_analysis(crash_data)
        
        # Cache result
        self.crash_cache[crash_hash] = {
            'crash_data': crash_data,
            'analysis': analysis
        }
        
        return analysis
    
    def _hash_crash(self, crash_data):
        # Create hash based on stack trace and registers
        stack_trace = crash_data.get('stack_trace', '')
        registers = crash_data.get('registers', {})
        
        import hashlib
        content = f"{stack_trace}:{registers}"
        return hashlib.md5(content.encode()).hexdigest()
```

---

## ðŸ“Š **Performance Monitoring**

### **Application Performance Monitoring**
```python
# APM integration
import time
from functools import wraps

class PerformanceMonitor:
    def __init__(self):
        self.metrics = {}
        self.alerts = []
    
    def monitor_function(self, func_name=None):
        def decorator(func):
            name = func_name or func.__name__
            
            @wraps(func)
            def wrapper(*args, **kwargs):
                start_time = time.time()
                try:
                    result = func(*args, **kwargs)
                    self._record_success(name, time.time() - start_time)
                    return result
                except Exception as e:
                    self._record_error(name, time.time() - start_time, str(e))
                    raise
            return wrapper
        return decorator
    
    def _record_success(self, func_name, duration):
        if func_name not in self.metrics:
            self.metrics[func_name] = {
                'calls': 0,
                'total_time': 0,
                'errors': 0,
                'avg_time': 0
            }
        
        self.metrics[func_name]['calls'] += 1
        self.metrics[func_name]['total_time'] += duration
        self.metrics[func_name]['avg_time'] = (
            self.metrics[func_name]['total_time'] / 
            self.metrics[func_name]['calls']
        )
        
        # Alert on slow performance
        if duration > 10:  # 10 seconds
            self.alerts.append({
                'type': 'slow_function',
                'function': func_name,
                'duration': duration,
                'timestamp': time.time()
            })
```

### **System Resource Monitoring**
```python
# Monitor system resources
import psutil
import time

class ResourceMonitor:
    def __init__(self):
        self.history = []
        self.monitoring = True
    
    def start_monitoring(self):
        while self.monitoring:
            metrics = {
                'timestamp': time.time(),
                'cpu_percent': psutil.cpu_percent(interval=1),
                'memory_percent': psutil.virtual_memory().percent,
                'disk_percent': psutil.disk_usage('/').percent,
                'network_io': psutil.net_io_counters(),
                'process_count': len(psutil.pids())
            }
            
            self.history.append(metrics)
            
            # Keep only last 1000 entries
            if len(self.history) > 1000:
                self.history.pop(0)
            
            time.sleep(60)  # Monitor every minute
    
    def get_performance_summary(self):
        if not self.history:
            return {}
        
        recent_metrics = self.history[-60:]  # Last hour
        
        return {
            'avg_cpu': sum(m['cpu_percent'] for m in recent_metrics) / len(recent_metrics),
            'avg_memory': sum(m['memory_percent'] for m in recent_metrics) / len(recent_metrics),
            'max_cpu': max(m['cpu_percent'] for m in recent_metrics),
            'max_memory': max(m['memory_percent'] for m in recent_metrics),
            'current_metrics': recent_metrics[-1] if recent_metrics else {}
        }
```

---

## ðŸ”§ **Configuration Optimization**

### **Production Configuration**
```python
# Optimized production configuration
class ProductionConfig:
    # Database optimization
    SQLALCHEMY_DATABASE_URI = 'postgresql://user:pass@localhost/shadowseek'
    SQLALCHEMY_ENGINE_OPTIONS = {
        'pool_size': 20,
        'max_overflow': 30,
        'pool_timeout': 30,
        'pool_recycle': 3600,
        'pool_pre_ping': True
    }
    
    # Redis configuration
    REDIS_URL = 'redis://localhost:6379/0'
    CACHE_TYPE = 'redis'
    CACHE_DEFAULT_TIMEOUT = 300
    
    # Worker configuration
    CELERY_BROKER_URL = 'redis://localhost:6379/1'
    CELERY_RESULT_BACKEND = 'redis://localhost:6379/2'
    CELERY_WORKER_CONCURRENCY = 4
    
    # Performance settings
    JSON_SORT_KEYS = False
    JSONIFY_PRETTYPRINT_REGULAR = False
    SEND_FILE_MAX_AGE_DEFAULT = 31536000  # 1 year
    
    # Security settings
    SESSION_COOKIE_SECURE = True
    SESSION_COOKIE_HTTPONLY = True
    SESSION_COOKIE_SAMESITE = 'Lax'
    
    # Logging
    LOG_LEVEL = 'INFO'
    LOG_FORMAT = '%(asctime)s %(levelname)s %(name)s: %(message)s'
```

### **Environment-Specific Tuning**
```bash
# Production environment variables
export FLASK_ENV=production
export WERKZEUG_RUN_MAIN=true
export OMP_NUM_THREADS=4
export MALLOC_ARENA_MAX=2

# System-level optimizations
echo never > \
  /sys/kernel/mm/transparent_hugepage/enabled
echo 1 > \
  /proc/sys/vm/overcommit_memory
echo 50 > \
  /proc/sys/vm/swappiness

# Network optimizations
echo 65536 > \
  /proc/sys/net/core/rmem_max
echo 65536 > \
  /proc/sys/net/core/wmem_max
echo 1024 65536 > \
  /proc/sys/net/ipv4/ip_local_port_range
```

---

## ðŸš€ **Deployment Optimization**

### **Container Optimization**
```dockerfile
# Optimized Docker configuration
FROM python:3.11-slim

# Install system dependencies
RUN apt-get update && \
    apt-get install -y \
      gcc \
      g++ && \
    rm -rf /var/lib/apt/lists/*

# Optimize Python
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONHASHSEED=random

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . /app
WORKDIR /app

# Optimize for production
RUN python -m compileall .
RUN python -O -m compileall .

# Use gunicorn for production
CMD ["gunicorn", \
     "--bind", "0.0.0.0:5000", \
     "--workers", "4", \
     "--worker-class", "gevent", \
     "--worker-connections", "1000", \
     "app:app"]
```

### **Load Balancer Configuration**
```nginx
# Nginx configuration for load balancing
upstream shadowseek_backend {
    server 127.0.0.1:5000 weight=1 max_fails=3 fail_timeout=30s;
    server 127.0.0.1:5001 weight=1 max_fails=3 fail_timeout=30s;
    server 127.0.0.1:5002 weight=1 max_fails=3 fail_timeout=30s;
}

server {
    listen 80;
    server_name shadowseek.local;
    
    # Static file serving
    location /static/ {
        root /var/www/shadowseek;
        expires 1y;
        add_header Cache-Control "public, immutable";
    }
    
    # API endpoints
    location /api/ {
        proxy_pass http://shadowseek_backend;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header Host $host;
        proxy_connect_timeout 5s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
    }
    
    # Frontend
    location / {
        root /var/www/shadowseek/build;
        try_files $uri $uri/ /index.html;
    }
}
```

Remember to continuously monitor performance metrics and adjust configurations based on your specific workload and infrastructure requirements. 